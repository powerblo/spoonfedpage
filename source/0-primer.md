---
share: true
---
#notes
[[i-vector-spaces|Vector Spaces ->]]
# Primer : A Few Tales
Let us begin with a few stories to distract ourselves from the material we should be covering. The first story is about two graduate students - let's call them Larry and Sergey - who were interesting in answering the question "given a bunch of books and a network of references connecting them, can you figure out which book is the most important?" This problem isn't as straightforward as it might initially seem (even if A is only referenced by a single book B, if B is important then so should be A), especially so if we're dealing with a library of possibly millions of books. It turned out however, that the complicated problem was just a rephrasing of a so called "eigenvector problem", one of the two major pillars of what you learn in "linear algebra". It shouldn't take you by surprise too much that these two graduate students were Larry Page and Sergey Brin, who used this knowledge to found Google by ranking websites based on how "important" they are with their [[1-pagerank|PageRank]] algorithm; the rest is history.

Next is an anecdote from the turbulent era of early modern physics, when cracks began to show in the centuries-old Newtonian mechanics and many were frantically trying to pick up the pieces. One such physicist was Heisenberg, who discovered that the ever elusive spectral lines of hydrogen could be understood using a system of bizzare multiplication laws. As Born would later point out however, these could be described by using matrices, the bread and butter of "linear algebra". Interestingly enough, Heisenberg's [[1-qmech|"matrix mechanics"]] agreed with another independently developed "wave mechanics", using the language of matrices to solve a certain Schroedinger's equation. The two together forms the foundations of modern physics, and Heisenberg was awarded the Nobel Prize for 1933 for his contributions. Born unfortunately was not, and Heisenberg profusely expressed his frustration of this; but this story has a happy ending, as Born too was awarded one in 1954.

For our last story, we look into the first modern meteorologist, an Englishman named Lewis Richardson. His dedication to the craft continued even as he fought in WWI at France; he developed working examples for his magnum opus between trasnporting the wounded, and lost his manuscript during the Battle of Champagne only to find it months later under a heap of coal. Even he however, armed with the sharpest methods maths had to offer, struggled for over six weeks to write a six hour weather forecast; which wasn't even accurate. He lamented, "perhaps some day... it will be possible to advance the computations faster than the weather... but this is a dream." This dream of his soon came true, when a team lead by the prolific von Neumann used the first electric computer ENIAC to truly begin the age of modern forecasts. What the two had in common however, was that they split up the atmosphere into a 'mesh of cells', and used methods like [[1-forecast|"finite differences" and "spectral models"]]; which both implicitly build upon "linear algebra".

## What is Linear Algebra?
Hopefully these three stories demonstrated to you that this "linear algebra" this is a big deal. In a nutshell, linear algebra is concerned with systems that behave as simply as you could realistically expect them to; ones whose output changes *proportionally* to how you change the input. 
This naive simplicity makes them extremely easy to calculate; computers are basically massive linear system solvers. Along with the not-so-naive insurance that a *lot* of things can be approximated to linear systems, it's not so surprising that linear algebra appears everywhere, be it data science, medicine, finance or particle physics.
Along with calculus, linear algebra is the lingua franca of mathematics; if you're interested in doing anything beyond reading from a manual and punching numbers, linear algebra is a no-brainer.
A similar aspect is that it's a crossingroads between an immensely wide variety of fields of math. One could argue that a good half of modern math is involved with taking a hard problem and relating it to a well-known result in linear algebra. Naturally its ideas and methods have been developed into many of maths' most prominent fields, deliberately so or not.

Then what can you expect to learn in a typical linear algebra course? The main goal of any introductory (or even advanced, depending on your perspective!) course on linear algebra is to investigate two kinds of problems; the **system of linear equations** ($Ax = b$), and the **eigenvector equation** ($Ax = \lambda x$).
The **system of linear equations** will be familar to many who remembers grade school maths; given a bunch of equations in the form $ax+by+\cdots=c$, you're asked to find the values of $x, y,\cdots$ Solving this by hand isn't particularly difficult if not tedious, and formalising this process doesn't lead to any interesting ideas or deeper problems (like for example, what algorithm computers use to do the same thing). This doesn't mean there's no merit in reviewing this step by step however; for one, it's great practice to get used to the standard notation, in how to express and manipulate linear systems. But more importantly, it allows us to formalise the intuitive idea of "which systems have solutions".
The **eigenvector equation** is a little more elusive, and is best left to after a solid understanding of matrices and what they represent, but we can still illuminate its significance. The rule of thumb for tackling a complicated problem is to ask "what simple behaviour within this problem can we focus on?" It happens that the eigenvector equation asks precisely that; it asks "what $x$ is linearly scaled (by a factor of $\lambda$) when acted on by $A$?" Given that we know these $x$, we can **diagonalise** the matrix $A$ and drastically simplify a problem, but also obtain key insights about the system at hand. Listing applications of the eigenvector equation and diagonalisation would be a significant task - finding closed forms of recursions, signal analysis and even quantum mechanics - precisely because it contains the technical details to utilise linear algebra in its full potential.

The remaining topics are natural extensions of linear algebra; by introducing the intuitively geometric concept of angles, **inner product spaces** provide 

[[i-vector-spaces|Vector Spaces ->]]